{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "import numpy as np\n",
    "data = pd.read_csv('nonlinear_rg.csv')\n",
    "data.drop('date', axis = 1, inplace = True)\n",
    "\n",
    "X = data.loc[:, 'a':'b']\n",
    "Y = data['y']\n",
    "X_train, Y_train = X[:-120], Y[:-120]\n",
    "X_test, Y_test = X[-120:], Y[-120:]\n",
    "X_train['exp(a)'] = np.exp(X_train['a'])\n",
    "X_train['sqrt(b)'] = np.sqrt(abs(X_train['b']))\n",
    "X_test['exp(a)'] = np.exp(X_test['a'])\n",
    "X_test['sqrt(b)'] = np.sqrt(abs(X_test['b']))\n",
    "scale = StandardScaler()\n",
    "X_train = scale.fit_transform(X_train)\n",
    "X_test = scale.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1706/1706 [==============================] - 3s - loss: 0.9815     \n",
      "Epoch 2/50\n",
      "1706/1706 [==============================] - 2s - loss: 0.9331     \n",
      "Epoch 3/50\n",
      "1706/1706 [==============================] - 2s - loss: 0.2381     \n",
      "Epoch 4/50\n",
      "1706/1706 [==============================] - 2s - loss: 0.1542     \n",
      "Epoch 5/50\n",
      "1706/1706 [==============================] - 2s - loss: 0.1262     \n",
      "Epoch 6/50\n",
      "1706/1706 [==============================] - 2s - loss: 0.1200     \n",
      "Epoch 7/50\n",
      "1706/1706 [==============================] - 3s - loss: 0.1106     \n",
      "Epoch 8/50\n",
      "1706/1706 [==============================] - 2s - loss: 0.1138     \n",
      "Epoch 9/50\n",
      "1706/1706 [==============================] - 2s - loss: 0.1087     \n",
      "Epoch 10/50\n",
      "1706/1706 [==============================] - 2s - loss: 0.1079     \n",
      "Epoch 11/50\n",
      "1706/1706 [==============================] - 2s - loss: 0.1056     \n",
      "Epoch 12/50\n",
      "1706/1706 [==============================] - 2s - loss: 0.1057     \n",
      "Epoch 13/50\n",
      "1706/1706 [==============================] - 2s - loss: 0.1041     \n",
      "Epoch 14/50\n",
      "1706/1706 [==============================] - 2s - loss: 0.1064     \n",
      "Epoch 15/50\n",
      "1706/1706 [==============================] - 2s - loss: 0.1065     \n",
      "Epoch 16/50\n",
      " 247/1706 [===>..........................] - ETA: 1s - loss: 0.1085"
     ]
    }
   ],
   "source": [
    "def reg_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(5, input_dim = 4, kernel_initializer = 'random_normal', activation = 'relu'))\n",
    "    model.add(Dense(20,  kernel_initializer = 'random_normal', activation = 'linear'))\n",
    "    model.add(Dense(12,  kernel_initializer = 'random_normal', activation = 'linear'))\n",
    "    model.add(Dense(1,  kernel_initializer = 'random_normal', activation = 'linear'))\n",
    "    model.compile(loss = 'mean_absolute_error', optimizer = 'sgd')\n",
    "    return model\n",
    "\n",
    "model = KerasRegressor(build_fn = reg_model, epochs = 50, batch_size = 1,verbose=1)\n",
    "model.fit(X_train,Y_train)\n",
    "\n",
    "\n",
    "print('точность: ', mae(model.predict(X_test), Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "120/120 [==============================] - 1s - loss: 1.1196     \n",
      "Epoch 2/50\n",
      "120/120 [==============================] - 0s - loss: 1.0529     \n",
      "Epoch 3/50\n",
      "120/120 [==============================] - 0s - loss: 1.0512     \n",
      "Epoch 4/50\n",
      "120/120 [==============================] - 0s - loss: 1.0530     \n",
      "Epoch 5/50\n",
      "120/120 [==============================] - 0s - loss: 1.0519     \n",
      "Epoch 6/50\n",
      "120/120 [==============================] - 0s - loss: 1.0501     \n",
      "Epoch 7/50\n",
      "120/120 [==============================] - 0s - loss: 1.0518     \n",
      "Epoch 8/50\n",
      "120/120 [==============================] - 0s - loss: 1.0504     \n",
      "Epoch 9/50\n",
      "120/120 [==============================] - 0s - loss: 1.0493     \n",
      "Epoch 10/50\n",
      "120/120 [==============================] - 0s - loss: 1.0475     \n",
      "Epoch 11/50\n",
      "120/120 [==============================] - 0s - loss: 1.0455     \n",
      "Epoch 12/50\n",
      "120/120 [==============================] - 0s - loss: 1.0403     \n",
      "Epoch 13/50\n",
      "120/120 [==============================] - 0s - loss: 1.0276     \n",
      "Epoch 14/50\n",
      "120/120 [==============================] - 0s - loss: 0.9987     \n",
      "Epoch 15/50\n",
      "120/120 [==============================] - 0s - loss: 0.8602     \n",
      "Epoch 16/50\n",
      "120/120 [==============================] - 0s - loss: 0.4734     \n",
      "Epoch 17/50\n",
      "120/120 [==============================] - 0s - loss: 0.3211     \n",
      "Epoch 18/50\n",
      "120/120 [==============================] - 0s - loss: 0.2626     \n",
      "Epoch 19/50\n",
      "120/120 [==============================] - 0s - loss: 0.2447     \n",
      "Epoch 20/50\n",
      "120/120 [==============================] - 0s - loss: 0.2185     \n",
      "Epoch 21/50\n",
      "120/120 [==============================] - 0s - loss: 0.2348     \n",
      "Epoch 22/50\n",
      "120/120 [==============================] - 0s - loss: 0.2384     \n",
      "Epoch 23/50\n",
      "120/120 [==============================] - 0s - loss: 0.2316     \n",
      "Epoch 24/50\n",
      "120/120 [==============================] - 0s - loss: 0.2285     \n",
      "Epoch 25/50\n",
      "120/120 [==============================] - 0s - loss: 0.2116     \n",
      "Epoch 26/50\n",
      "120/120 [==============================] - 0s - loss: 0.2131     \n",
      "Epoch 27/50\n",
      "120/120 [==============================] - 0s - loss: 0.2332     \n",
      "Epoch 28/50\n",
      "120/120 [==============================] - 0s - loss: 0.2249     \n",
      "Epoch 29/50\n",
      "120/120 [==============================] - 0s - loss: 0.2208     \n",
      "Epoch 30/50\n",
      "120/120 [==============================] - 0s - loss: 0.2139     \n",
      "Epoch 31/50\n",
      "120/120 [==============================] - 0s - loss: 0.2253     \n",
      "Epoch 32/50\n",
      "120/120 [==============================] - 0s - loss: 0.2044     \n",
      "Epoch 33/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.226 - 0s - loss: 0.2186     \n",
      "Epoch 34/50\n",
      "120/120 [==============================] - 0s - loss: 0.2080     \n",
      "Epoch 35/50\n",
      "120/120 [==============================] - 0s - loss: 0.2120     \n",
      "Epoch 36/50\n",
      "120/120 [==============================] - 0s - loss: 0.2197     \n",
      "Epoch 37/50\n",
      "120/120 [==============================] - 0s - loss: 0.1995     \n",
      "Epoch 38/50\n",
      "120/120 [==============================] - 0s - loss: 0.2221     \n",
      "Epoch 39/50\n",
      "120/120 [==============================] - 0s - loss: 0.2238     \n",
      "Epoch 40/50\n",
      "120/120 [==============================] - 0s - loss: 0.2125     \n",
      "Epoch 41/50\n",
      "120/120 [==============================] - 0s - loss: 0.2228     \n",
      "Epoch 42/50\n",
      "120/120 [==============================] - 0s - loss: 0.2027     \n",
      "Epoch 43/50\n",
      "120/120 [==============================] - 0s - loss: 0.2162     \n",
      "Epoch 44/50\n",
      "120/120 [==============================] - 0s - loss: 0.2141     \n",
      "Epoch 45/50\n",
      "120/120 [==============================] - 0s - loss: 0.1882     \n",
      "Epoch 46/50\n",
      "120/120 [==============================] - 0s - loss: 0.2178     \n",
      "Epoch 47/50\n",
      "120/120 [==============================] - 0s - loss: 0.2132     \n",
      "Epoch 48/50\n",
      "120/120 [==============================] - 0s - loss: 0.2108     \n",
      "Epoch 49/50\n",
      "120/120 [==============================] - 0s - loss: 0.1915     \n",
      "Epoch 50/50\n",
      "120/120 [==============================] - 0s - loss: 0.2142     \n",
      "точность:  0.162389026919\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(5, input_dim = 4, kernel_initializer = 'random_normal', activation = 'relu'))\n",
    "model.add(Dense(20,  kernel_initializer = 'random_normal', activation = 'linear'))\n",
    "model.add(Dense(12,  kernel_initializer = 'random_normal', activation = 'linear'))\n",
    "model.add(Dense(1,  kernel_initializer = 'random_normal', activation = 'linear'))\n",
    "model.compile(loss = 'mean_absolute_error', optimizer = 'sgd')\n",
    "model.fit(X_test,Y_test, epochs = 50, batch_size = 1)\n",
    "print('точность: ', mae(model.predict(X_test), Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1706/1706 [==============================] - 5s - loss: 1.2031 - dense_30_loss: 0.7713 - output_1_loss: 0.7881 - output_2_loss: 0.9770     \n",
      "Epoch 2/30\n",
      "1706/1706 [==============================] - 4s - loss: 0.7644 - dense_30_loss: 0.4178 - output_1_loss: 0.5527 - output_2_loss: 0.9039     - ETA: 3s - loss: 0.7244 - dense_\n",
      "Epoch 3/30\n",
      "1706/1706 [==============================] - 4s - loss: 0.7639 - dense_30_loss: 0.4178 - output_1_loss: 0.5604 - output_2_loss: 0.8896     \n",
      "Epoch 4/30\n",
      "1706/1706 [==============================] - 4s - loss: 0.7642 - dense_30_loss: 0.4184 - output_1_loss: 0.5600 - output_2_loss: 0.8893     \n",
      "Epoch 5/30\n",
      "1706/1706 [==============================] - 4s - loss: 0.7568 - dense_30_loss: 0.4118 - output_1_loss: 0.5600 - output_2_loss: 0.8851     \n",
      "Epoch 6/30\n",
      "1706/1706 [==============================] - 4s - loss: 0.7485 - dense_30_loss: 0.4048 - output_1_loss: 0.5541 - output_2_loss: 0.8877     - ETA: 3s - loss: 0.7572\n",
      "Epoch 7/30\n",
      "1706/1706 [==============================] - 4s - loss: 0.7506 - dense_30_loss: 0.4067 - output_1_loss: 0.5543 - output_2_loss: 0.8882     \n",
      "Epoch 8/30\n",
      "1706/1706 [==============================] - 4s - loss: 0.7547 - dense_30_loss: 0.4110 - output_1_loss: 0.5567 - output_2_loss: 0.8834     - ETA: 0s - loss: 0.7489 - dense_30_loss: 0.4073 - output_1_loss: 0.5498 - output_2_lo\n",
      "Epoch 9/30\n",
      "1706/1706 [==============================] - 4s - loss: 0.7651 - dense_30_loss: 0.4202 - output_1_loss: 0.5596 - output_2_loss: 0.8854     \n",
      "Epoch 10/30\n",
      "1706/1706 [==============================] - 4s - loss: 0.7691 - dense_30_loss: 0.4234 - output_1_loss: 0.5628 - output_2_loss: 0.8842     \n",
      "Epoch 11/30\n",
      "1706/1706 [==============================] - 4s - loss: 0.7766 - dense_30_loss: 0.4354 - output_1_loss: 0.5465 - output_2_loss: 0.8862     \n",
      "Epoch 12/30\n",
      "1706/1706 [==============================] - 4s - loss: 0.7720 - dense_30_loss: 0.4260 - output_1_loss: 0.5625 - output_2_loss: 0.8860     \n",
      "Epoch 13/30\n",
      "1706/1706 [==============================] - 4s - loss: 0.7577 - dense_30_loss: 0.4111 - output_1_loss: 0.5651 - output_2_loss: 0.8853     \n",
      "Epoch 14/30\n",
      "1706/1706 [==============================] - 4s - loss: 0.7866 - dense_30_loss: 0.4395 - output_1_loss: 0.5669 - output_2_loss: 0.8852     \n",
      "Epoch 15/30\n",
      "1706/1706 [==============================] - 4s - loss: 0.7583 - dense_30_loss: 0.4129 - output_1_loss: 0.5606 - output_2_loss: 0.8860     \n",
      "Epoch 16/30\n",
      "1706/1706 [==============================] - 5s - loss: 0.7893 - dense_30_loss: 0.4413 - output_1_loss: 0.5690 - output_2_loss: 0.8868     \n",
      "Epoch 17/30\n",
      "1706/1706 [==============================] - 4s - loss: 0.7613 - dense_30_loss: 0.4131 - output_1_loss: 0.5689 - output_2_loss: 0.8873     \n",
      "Epoch 18/30\n",
      "1706/1706 [==============================] - 4s - loss: 0.7979 - dense_30_loss: 0.4494 - output_1_loss: 0.5708 - output_2_loss: 0.8866     \n",
      "Epoch 19/30\n",
      "1706/1706 [==============================] - 4s - loss: 0.8009 - dense_30_loss: 0.4491 - output_1_loss: 0.5809 - output_2_loss: 0.8876     - ETA: 3s - loss: 0.8385 -\n",
      "Epoch 20/30\n",
      "1706/1706 [==============================] - 4s - loss: 0.7902 - dense_30_loss: 0.4416 - output_1_loss: 0.5707 - output_2_loss: 0.8865     \n",
      "Epoch 21/30\n",
      "1706/1706 [==============================] - 4s - loss: 0.7772 - dense_30_loss: 0.4261 - output_1_loss: 0.5803 - output_2_loss: 0.8848     \n",
      "Epoch 22/30\n",
      "1706/1706 [==============================] - 4s - loss: 0.7782 - dense_30_loss: 0.4272 - output_1_loss: 0.5791 - output_2_loss: 0.8865     \n",
      "Epoch 23/30\n",
      "1706/1706 [==============================] - 4s - loss: 0.7969 - dense_30_loss: 0.4448 - output_1_loss: 0.5818 - output_2_loss: 0.8879     \n",
      "Epoch 24/30\n",
      "1706/1706 [==============================] - 4s - loss: 0.7908 - dense_30_loss: 0.4412 - output_1_loss: 0.5743 - output_2_loss: 0.8864     \n",
      "Epoch 25/30\n",
      "1706/1706 [==============================] - 3s - loss: 0.7888 - dense_30_loss: 0.4382 - output_1_loss: 0.5777 - output_2_loss: 0.8862     \n",
      "Epoch 26/30\n",
      "1706/1706 [==============================] - 3s - loss: 0.7825 - dense_30_loss: 0.4328 - output_1_loss: 0.5741 - output_2_loss: 0.8869     \n",
      "Epoch 27/30\n",
      "1706/1706 [==============================] - 3s - loss: 0.8169 - dense_30_loss: 0.4619 - output_1_loss: 0.5921 - output_2_loss: 0.8866     - ETA: 2s - loss: 0.8591 - dense_30_loss: 0.4995 - out\n",
      "Epoch 28/30\n",
      "1706/1706 [==============================] - 3s - loss: 0.7892 - dense_30_loss: 0.4403 - output_1_loss: 0.5713 - output_2_loss: 0.8876     \n",
      "Epoch 29/30\n",
      "1706/1706 [==============================] - 3s - loss: 0.7989 - dense_30_loss: 0.4442 - output_1_loss: 0.5906 - output_2_loss: 0.8873     \n",
      "Epoch 30/30\n",
      "1706/1706 [==============================] - 3s - loss: 0.7912 - dense_30_loss: 0.4391 - output_1_loss: 0.5817 - output_2_loss: 0.8883     - ETA: 3s - loss: 0.8412 - den\n",
      "точность:  0.918561378562\n"
     ]
    }
   ],
   "source": [
    "#модель с 2-мя входами 3-мя выходами\n",
    "\n",
    "from keras.layers import Input, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "input_1 = Input(shape = (2, ) , dtype = 'float32', name='input_1')\n",
    "layer_1 = Dense(3, kernel_initializer = 'random_normal', activation = 'relu')(input_1)\n",
    "layer_1 = Dense(20,  kernel_initializer = 'random_normal', activation = 'linear')(layer_1)\n",
    "output_1 = Dense(1, activation='linear', name = 'output_1')(layer_1)\n",
    "\n",
    "\n",
    "input_2 = Input(shape = (2, ), dtype = 'float32', name = 'input_2')\n",
    "layer_2 = Dense(3, kernel_initializer = 'random_normal', activation = 'relu')(input_2)\n",
    "layer_2 = Dense(20,  kernel_initializer = 'random_normal', activation = 'linear')(layer_2)\n",
    "output_2 = Dense(1, activation='linear', name = 'output_2')(layer_2)\n",
    "\n",
    "x = concatenate([layer_1, layer_2])\n",
    "x = Dense(15, activation='linear')(x)\n",
    "x = Dense(1,  kernel_initializer = 'random_normal', activation = 'linear')(x)\n",
    "\n",
    "model = Model(inputs=[input_1, input_2], outputs = [x, output_1, output_2])\n",
    "\n",
    "model.compile(loss = 'mean_absolute_error', optimizer = 'sgd', loss_weights=[1., .3, .2])\n",
    "\n",
    "model.fit([X_train[:, [0, 2]], X_train[:, [1, 3]]], [Y_train, Y_train, Y_train], epochs = 30, batch_size = 1)\n",
    "\n",
    "print('точность: ', mae(np.array(model.predict([X_test[:, [0, 2]], X_test[:, [1, 3]]]))[2], Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan]], dtype=float32), array([[ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan]], dtype=float32), array([[ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan]], dtype=float32)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([X_test[:, [0, 2]], X_test[:, [1, 3]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
